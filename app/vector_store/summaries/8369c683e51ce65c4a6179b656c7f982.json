{
  "doc_hash": "8369c683e51ce65c4a6179b656c7f982",
  "summary": "This document presents a groundbreaking research paper titled \"Attention Is All You Need\" introducing the Transformer, a novel neural network architecture for sequence transduction tasks, particularly machine translation. Key points include:\n\n1. Architecture:\n- Based solely on attention mechanisms, eliminating recurrence and convolutions\n- Encoder-decoder structure with 6 identical layers each\n- Uses multi-head attention, scaled dot-product attention, and position-wise feed-forward networks\n\n2. Performance:\n- Achieves state-of-the-art results on English-to-German (28.4 BLEU) and English-to-French (41.0 BLEU) translation tasks\n- Outperforms previous models while being more parallelizable and faster to train\n- Single model with 165 million parameters\n\n3. Key innovations:\n- Self-attention mechanism allows modeling dependencies between input and output sequences\n- Positional encoding using sine and cosine functions to incorporate sequence order information\n- Multi-head attention enables attending to information from different representation subspaces\n\n4. Advantages:\n- Constant number of sequentially executed operations (O(1)) for long-range dependencies\n- Lower computational complexity for typical sequence lengths in NLP tasks\n- More interpretable model with attention heads learning different tasks\n\n5. Training details:\n- Used WMT 2014 English-German and English-French datasets\n- Trained on 8 NVIDIA P100 GPUs for 12 hours (base model) to 3.5 days (large model)\n- Employed Adam optimizer, label smoothing, and residual dropout\n\n6. Impact:\n- Represents a significant shift in approach to sequence transduction\n- Authors express excitement about future applications beyond text (images, audio, video)\n- Code available on GitHub under tensor2tensor repository\n\nThis paper introduces a revolutionary architecture that has significantly impacted the field of natural language processing and beyond.",
  "metadata": {
    "producer": "PyPDF2",
    "creator": "PyPDF",
    "creationdate": "",
    "subject": "Neural Information Processing Systems http://nips.cc/",
    "publisher": "Curran Associates, Inc.",
    "language": "en-US",
    "created": "2017",
    "eventtype": "Poster",
    "description-abstract": "The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.",
    "title": "Attention is All you Need",
    "date": "2017",
    "moddate": "2018-02-12T21:22:10-08:00",
    "published": "2017",
    "type": "Conference Proceedings",
    "firstpage": "5998",
    "book": "Advances in Neural Information Processing Systems 30",
    "description": "Paper accepted and presented at the Neural Information Processing Systems Conference (http://nips.cc/)",
    "editors": "I. Guyon and U.V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett",
    "author": "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, ≈Åukasz Kaiser, Illia Polosukhin",
    "lastpage": "6008",
    "source": "agent\\documents\\NIPS-2017-attention-is-all-you-need-Paper.pdf",
    "total_pages": 11,
    "page": 0,
    "page_label": "1"
  },
  "original_length": 32651,
  "summary_length": 1912
}