<!--
from nltk.corpus.reader import documentsfrom agent_container.agent.core.rag_agent import RAGAgentfrom agent_container.agent.graph.workflow import create_workflow
-->


# ğŸ§  RAG Agent using LangGraph & LangChain

This project is a **Retrieval-Augmented Generation (RAG) Agent** with **Reinforcement Learning Loop** built on:

- ğŸ” **LangGraph**: for orchestrating recursive retrieval using conditional edges  
- ğŸ¦œ **LangChain**: to integrate vector stores, embeddings, memory, and agents  
- ğŸ§­ **Tavily Search**: to augment answers with live web results  
- ğŸ¤– **Ollama Local Models**: as the core LLM (e.g., LLaMA 3, KazLLM)  
- ğŸ” **PGVector**: for efficient local semantic retrieval  
- ğŸ’¡ **HuggingFace Embeddings**: to embed documents and queries for semantic search
- ğŸ¦¥ **Unsloth and TRL**: for Reinforcement Learning Loop performance boost and fine-tuning

---

## âœ¨ Features

- **Agent-centric architecture**: all logic encapsulated in a single `Agent` class
Note: for modulation purposes, main 'run' function has be depreciated.
- Basic interface:  
    ```python
    from agent import RAGAgent
    from agent import create_workflow

    agent = RAGAgent()
    graph = create_workflow(agent)
    
    # Init vectorstore
    documents_path = "/docs/path"
    agent.init_vectorstores(documents_path)
    
    # All initial state params can be taken from 
    # agent_container/agent/core/state.py
    # Note: question and documents_path are required
    initial_state = {...}
  
    # Run graph
    ans = graph.invoke(initial_state)
    ```
- Recursive document retrieval via **LangGraph** conditional edges
- Seamless integration with **Tavily Search API** for real-time information
- Modular design for easy **extensibility and integration**
- Feedback loop for automatic model adaptation
---

## ğŸ—‚ï¸ Project Structure
```graphql
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ log/
â”‚   â”‚   â””â”€â”€rag_agent.log    # Log file
â”‚   â”œâ”€â”€ .dockerignore
â”‚   â”œâ”€â”€ docker-compose.yaml     # Docker-compose for RAG agent container
â”‚   â”œâ”€â”€ requrements.txt
â”‚   â”œâ”€â”€ vector_store    # Vector stores folder. Will be created on initialization
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ agent/          # Main Agent module: query processing & orchestration
â”‚       â”œâ”€â”€ docs/      # Source documents to embed
â”‚       â”œâ”€â”€ utils/
â”‚       â”‚   â”œâ”€â”€ document_processor.py   # Read/chunk documents tool
â”‚       â”‚   â”œâ”€â”€ summary_manager.py      # Summary generating tool
â”‚       â”‚   â”œâ”€â”€ query_analyzer.py       # Query analysis and breakdown tool
â”‚       â”‚   â””â”€â”€ logging_config.py       # Logging setup
â”‚       â”œâ”€â”€ store/
â”‚       â”‚   â””â”€â”€ vector_store.py     # FAISS-based vector store logic using LangChain
â”‚       â”œâ”€â”€ graph/
â”‚       â”‚   â””â”€â”€ workflow.py     # LangGraph setup with conditional edges for response
â”‚       â”‚   â””â”€â”€ rl_workflow.py     # LangGraph setup with conditional edges for RLHF
â”‚       â”œâ”€â”€ rl/
â”‚       â”‚   â”œâ”€â”€ art.py       # ARTe module for trajectories
â”‚       â”‚   â”œâ”€â”€ entry.py     # Entries for RLHF
â”‚       â”‚   â””â”€â”€ training.py  # Main training loop
â”‚       â””â”€â”€ core/
â”‚           â”œâ”€â”€ config.py       # Pydantic config facade
â”‚           â”œâ”€â”€ state.py        # Graph variables/states
â”‚           â”œâ”€â”€ agent.py        # Agent class
â”‚           â””â”€â”€ settings/
â”‚                   â”œâ”€â”€ .env.template   # Helps to locate .env file
â”‚                   â””â”€â”€ .env            # API keys, settings, and constants
â”‚
â”œâ”€â”€ .env  # .env for chainlit key and database
â”œâ”€â”€ .env.template   # Helps to locate .env file
â”œâ”€â”€ requirements.txt    # Main requirements file to setup venv and run code locally
â”œâ”€â”€ .gitignore
â”œâ”€â”€ graphics/           # Image folder
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started
### 1. Install Requirements
```bash
pip install -r requirements.txt
```
Make sure the following packages are included:
- ```langchain```
- ```langgraph```
- ```pyscopg2```
- ```huggingface_hub```
- ```ollama```
- ```unsloth```
- ```trl```
- ```transformers```

Will be in the future version, now we use LangChain module TavilySearch
- ```tavily-python```

### 2. Set Environment Variables
1. Paste your variables into .env that can be created in **the same folder and the same format** as _.env.template_

### 3. Set up Chainlit
1. Go to /app and open the console
2. Write ```chainlit create-secret```
3. Copy ```CHAINLIT_AUTH_SECRET=...``` into your upper .env file
4. Write ```DATABASE_URL=postgresql://<username>:<password>@<host>:<port>/<dbname>``` into your upper .env file
   
### 4. Make sure documents are set up
1. Create folder /docs
2. Add all documents you want to use

### 5. Start app
1. If you want to start in containers
   1. Go to _/app_
   2. Open console and write: ```docker compose up```
2. If you want to start it locally:
   1. Go to _/root_ folder
   2. Open console and write: ```chainlit run app/app.py```
---

## ğŸ§  Workflow

- Uses **LangChain PGVector** to perform local document retrieval
- Embeds documents and queries using **HuggingFace** embedding models
- Implements recursive retrieval with **LangGraph** conditional edge logic
- Falls back to **Tavily Search** when local results are insufficient
- Combines all context and queries **KazLLM/LLaMA 3** for final response and summaries
- Feedback is handled in **LangGraph** conditional edge logic with **CatBoost/Isolation Forest** models for filtration
- When enough feedback collected, starts retraining on **Unsloth**

---

## ğŸ› ï¸ Customization
All files are modules within my flow, so you there are no direct dependancies.
Follow functions names and return type, you can change every part.
For example:
- VectorStore can be differenct
- Workflow can be redirected or new nodes/edges initialized
- Document preprocessing customized and different extensions added
- Models and other hyperparameters changed

---

## ğŸ“Œ TODO
- Support multi-turn conversational memory
- Better evaluation technique (fuzzy match)
- Prompt Injection security

---
